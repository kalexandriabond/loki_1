{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Structure of the reward-learning task"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The reward-learning task independently manipulates two [facets of uncertainty](https://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.1001048). The first is [decision conflict](http://www.jneurosci.org/content/jneuro/28/13/3468.full.pdf), which is the degree to which different targets have similar reward probabilities. If the reward probabilities are close ($0.65$ and $0.35$), the process of choosing the target which gives you the most reward is harder; if the reward probabilities are vastly different ($0.80$ and $0.20$), then the decision is relatively easy. The second is [volatility](http://www.cs.colorado.edu/~mozer/Teaching/syllabi/SequentialDependencies/readings/Behrensetal2007.pdf), which is the rate at which the identity of the optimal target changes. The number of trials between identity changes is determined according to the $\\lambda$ parameter of a Poisson process. For example, when $\\lambda$ is low, then the identity of the optimal target changes frequently. Here are some samples of the reward structure: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![nb_imgs/reward_schedules.png](https://github.com/kmbond/loki_1/blob/master/nb_imgs/reward_schedules.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We plan to manipulate all combinations of three levels of each dimension of uncertainty. This results in nine conditions. The experiment is within-subject, so each participant experiences each condition. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![nb_imgs/task.png](nb_imgs/task.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We think that these different forms of uncertainty push the learner into different decision states expressed as the modulation of the rate of evidence accumulation ($v$) and the amount of evidence needed to make a decision ($a$) ([more](http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.696.3006&rep=rep1&type=pdf) on the drift diffusion modeling framework). We call these decision parameter profiles \"decision phenotypes\". Based on previous work, we predict that a relatively volatile or high conflict environment will encourage exploration, and a low conflict and low volatility env. will lead to exploitation. Decomposing decision-making behavior into these distinct parameters allows us to characterize different forms of exploration. For example, exploratory phenotype B is expressed as fast evidence accumulation and a low decision threshold, while Phenotype C is expressed as slow accumulation and a low decision threshold. <br><br>\n",
    "\n",
    "In our current experiment, we are collecting pupillometry data as a proxy for fluctuations in locus-coereleus noradrenergic (LC-NE) system activity. There is an abundance of evidence to suggest that LC-NE activity can act as a readout of adaptive [shifts in decision policy](http://www.wpic.pitt.edu/research/biometrics/SPR%202011%20Workshop/SPR%202011%20References/Aston-Jones%20&%20Cohen%20Loc%20Coer-NE%20annurev.neuro%202005.pdf), with phasic LC activation corresponding to exploitation and tonic activity related to exploration. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![nb_imgs/noradrenergic_policy.png](nb_imgs/noradrenergic_policy.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Subtasks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Criterion task** Two-alternative forced choice for male / female judgment. The purpose of this task is to minimize the impact of perceptual learning during early reward-learning. Participants are asked to select the greeble that they think is female and receive binary feedback about their selection (correct or incorrect) until they reach 95% accuracy. After some pilot testing, this happens after about only 2 minutes of training. <br>\n",
    "**Behavioral classification of the hierarchy of greeble identity** 2-alternative forced choice task for individual-level and family-level classification. Sex-level classification is conducted with feedback (above) so is excluded from this test. This task aims to measure the degree to which participants learn the greeble hierarchy via passive perceptual learning, so no feedback is given. As it stands now, participants will be tested on this twice; once as a baseline measure, prior to any extensive experience with the greebles, and once at the end of all nine reward-learning sessions.<br>\n",
    "**Test of representational similarity change with passive perceptual exposure** Passive exposure to greebles (perhaps with a button press when fixation cross appears) to test representational similarity change over time. One session will be dedicated to capturing a baseline measure and one session will be administered at the end, after the reward-learning sessions. <br>\n",
    "**Reward-learning task** The task as outlined above. Nine reward-learning sessions to test the impact of volatility and conflict on decision-making parameters. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# fMRI task sequence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Projected *N* = 4 subjects \n",
    "\n",
    "**Session 1**\n",
    "* Behavioral classification task  *(8 test greebles per family and 5 families, 40 total)* (now redundant) <br> \n",
    "* Representational similarity test *(only the 40 holdout greebles shown)* <br> \n",
    "\n",
    "**Sessions 2-10**\n",
    "* Criterion task during structural imaging **x9** *(40 training greebles)* [variable timing, dependent on when participants reach 95% accuracy criterion]   <br>\n",
    "* Reward-learning task **x9** *(40 training greebles)* [timing shown below]<br>\n",
    "    * moderate (rather than fast / slow) event-related design with an exponentially distributed ITI, ranging from 4-16 s\n",
    "    * 5 8-minute runs, each 80 trials long for a total of 400 trials per session\n",
    "    * 40 minutes per session\n",
    "    * 1.5 s TR\n",
    "    * ROI = basal ganglia \n",
    "    * Other details to be worked out with Tim\n",
    "    * Male / female greebles are shown as left/right decisions, as show above and below. These are different indiviudal greebles repeatedly drawn from different families. \n",
    "\n",
    "**Session 11**\n",
    "* Representational similarity test *(only the holdout 40 greebles shown)* [timing unknown? depends on parameters you and mike choose. May present each holdout individual twice, as mentioned in the meeting.]  <br>\n",
    "* Behavioral classification task *(40 test greebles)* [Originally, this was going to be 140 trials with a same-different family judgment. 50% of the trials would show individuals from the same family, and 50% would show individuals not from the same family, shuffled across trials. Now that Mike added the other layer (individual + the original family), might need to be longer and need to figure out a new sampling procedure.] <br>\n",
    "\n",
    "*Note that all of the training greebles are sampled with replacement. It's up to you and Mike for how you want to sample the test greebles! I think that Tim suggested showing each test greeble twice within a run?* "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task instructions, mixed with some parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* On each trial, participants will be asked to choose the highest-reward target. Then they will be shown the reward earned on each trial based on their decision. To prevent prepotent selections, the position of the rewarding target will be randomized across trials, with the target identity as the rewarding feature (rather than target position). \n",
    "* Participants will be required to respond within 0.75 s* (requires piloting. originally 1 s.) \n",
    "* Participants receive feedback via a reward bank centered between the greebles. This will be initially set to $t/2$ points. Each decision costs 1 coin. This bank is updated after each decision. \n",
    "* If they respond too quickly or too slowly, the reward bank will change color (too fast, too slow, or no response = red). If they earn points, the bank will turn green. If they lose points, then the bank will turn yellow.\n",
    "* The reward bank will act as a fixation point, which will be on for a variable ITI (as above, 2-8 s exponentially distributed)\n",
    "* Rewards come from a normal distribution, $N(\\mu=3, \\sigma=1)$\n",
    "* Participants make a left-right selection by pressing the left and right buttons of a button box with their thumbs (we may adjust this to work with the glove in the scanner). \n",
    "* All greebles used are symmetric and at the same viewing angle, with a 50/50 split into hold-out and test sets. Note that, right now, this split is constant across subjects. We could do something different (maybe something more like a $k$-fold cross-validation approach? with each subject getting a unique training and test set). Happy to get your feedback on that!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below are the instruction screens for the main reward-learning task. Excuse the weird proportions... I don't have access to the experimental computer at the moment, which has a different screen resolution. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![imgs/greeble_intro.png](nb_imgs/greeble_intro.png)\n",
    "![imgs/money_ask.png](nb_imgs/money_ask.png)\n",
    "![imgs/bank_intro.png](nb_imgs/bank_intro.png)\n",
    "![imgs/buttons_focus.png](nb_imgs/buttons_focus.png)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
